# ğŸš€ Interview Question Generator

    An advanced RAG (Retrieval-Augmented Generation) powered application that generates high-quality interview questions and answers from any technical document (PDF).

### ğŸ“Œ Overview

##### Interview Question Generator is a scalable, production-style AI application designed for:

    ğŸ‘¨â€ğŸ’¼ Recruiters preparing technical interviews
    ğŸ‘©â€ğŸ’» Candidates preparing from documentation
    ğŸ“š Anyone wanting structured Q&A from PDFs

###### It leverages Groq Llama 3.1, LangChain, and FAISS to deliver ultra-fast, context-aware results.

### ğŸ› ï¸ Tech Stack
##### Component	Technology Used
    ğŸ§  LLM	Groq Llama 3.1 8B
    ğŸ”— Orchestration	LangChain (LCEL)
    ğŸ“Š Vector Database	FAISS
    ğŸ“ Embeddings	HuggingFace (all-MiniLM-L6-v2)
    ğŸŒ Backend	Flask
    ğŸ¨ Frontend	HTML5 / CSS3 / Vanilla JavaScript
    âš™ï¸ How to Run Locally
    
### 1ï¸âƒ£ Create a Conda Environment
```
conda create -p interview python=3.13 -y
```
```
conda activate interview
```
### 2ï¸âƒ£ Install Requirements
```
pip install -r requirements.txt
```
### 3ï¸âƒ£ Install the Local Package

This step is crucial for resolving imports from the src/ directory:
```
pip install -e .
```
4ï¸âƒ£ Setup Environment Variables

### Create a .env file in the root directory and add your Groq API key:
GROQ_API_KEY=your_api_key_here

### 5ï¸âƒ£ Run the Application
```
python app.py
```
Then open your browser and go to:
```
http://127.0.0.1:8080
```
### ğŸ“‚ Project Structure
```
Interview_question_generator/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ artifacts/
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ vectorstore/
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ <project_name>/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ exception.py
â”‚       â”‚
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ data_ingestion.py
â”‚       â”‚   â”œâ”€â”€ data_transformation.py
â”‚       â”‚   â”œâ”€â”€ model_trainer.py
â”‚       â”‚   â””â”€â”€ model_config.py        # Centralized LLM & Embeddings configuration
â”‚       â”‚
â”‚       â”œâ”€â”€ pipelines/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ generation_pipeline.py
â”‚       â”‚   â””â”€â”€ rag_pipeline.py
â”‚       â”‚
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ prompts.py             # Prompt templates
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ result.html
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ main.js
â”‚
â”œâ”€â”€ research/
â”‚   â””â”€â”€ trials.ipynb                   # Notebook experiments
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .env
â””â”€â”€ README.md
```
### ğŸ§  Architecture Highlights

    ğŸ”¹components/ â†’ Core building blocks (data, model, configs)
    ğŸ”¹pipelines/ â†’ End-to-end workflows (Generation + RAG)
    ğŸ”¹utils/ â†’ Shared utilities like prompt templates
    ğŸ”¹vectorstore/ â†’ FAISS index storage
    ğŸ”¹artifacts/ â†’ Saved models / intermediate outputs
    ğŸ”¹research/ â†’ Experimental notebooks

#### The project follows a modular, industry-standard architecture ensuring scalability and maintainability.

### ğŸ§  Project Details & Process Flow
##### 1ï¸âƒ£ High-Performance Question Generation (Map-Reduce Strategy)

        -> To handle large PDFs efficiently:
             ğŸ”¹ Map Phase

        -> PDF is split into chunks

        -> Chunks are processed in parallel (multi-threading)

        -> Each chunk generates draft questions using Llama 3.1 on Groq
            ğŸ”¹ Reduce Phase

        -> Draft questions are aggregated

        -> Duplicates removed

### Final list refined to match user-requested count

    âœ”ï¸ Prevents context overflow
    âœ”ï¸ Ensures fast response time
    âœ”ï¸ Maintains quality control

##### 2ï¸âƒ£ Intelligent Answering (RAG Pipeline)

        -> Once questions are generated:
            ğŸ“Œ Step 1: Embeddings

        -> Text chunks are converted into numerical vectors using Sentence Transformers.
            ğŸ“Œ Step 2: Vector Search

        -> FAISS performs similarity search to find the most relevant document chunk.
            ğŸ“Œ Step 3: Contextual Answer Generation

        -> The LLM receives:
            ğŸ“Œ The relevant paragraph
            ğŸ“Œ The specific question

        -> This ensures:
            âŒ No hallucinations
            âœ… Context-grounded answers
             High accuracy
            ğŸŒ User Interface
            ğŸ“„ Upload PDF
            ğŸ”¢ Select number of questions
            âš¡ Real-time Q&A generation
            ğŸ’¾ Download generated Q&A as text file
            ğŸ§¹ Clean, responsive Flask-based UI.
            ğŸ”¥ Key Features
            âš¡ Ultra-fast inference with Groq
            ğŸ§  Map-Reduce parallel processing
            ğŸ“Š FAISS vector similarity search
            ğŸ“ PDF-based contextual understanding
            ğŸ’¡ Scalable modular architecture
            ğŸ¯ Hallucination-minimized answers
            ğŸ“ˆ Future Improvements
            â¬†ï¸ Add user authentication
            ğŸ“Š Admin analytics dashboard
            ğŸ—‚ï¸ Multiple document support
            â˜ï¸ Docker & Cloud deployment

### Contributions are welcome!
### Fork the repository
### Create your feature branch
### Commit your changes
### Open a Pull Request

### ğŸ“œ License
    This project is open-source and available under the MIT License.

# â­ If You Like This Project
    Give it a â­ on GitHub and share it with others!